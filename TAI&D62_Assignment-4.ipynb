{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4b1565",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ce69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Pravin\n",
      "[nltk_data]     Bhise\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance.\"\n",
    "print(\"Sentence tokenization: \")       \n",
    "sents = (sent_tokenize(text))        \n",
    "print(sents)\n",
    "\n",
    "print (\"Word tokenization: \", word_tokenize(text))\n",
    "\n",
    "\n",
    "print(\"Word tokenization with list of list of each word: \")\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fafac",
   "metadata": {},
   "source": [
    "# 2. Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c870986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance (text) .\"\n",
    "        \n",
    "custom_list = set(stopwords.words('english')+list(punctuation))        \n",
    "        \n",
    "word_list = [word for word in word_tokenize(text) if word not in custom_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828e5f5",
   "metadata": {},
   "source": [
    "# 3. N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', \\\n",
    "             'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n",
    "\n",
    "finde = BigramCollocationFinder.from_words(word_list)\n",
    "print(finde.ngram_fd.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7bed6",
   "metadata": {},
   "source": [
    "# 4. Word Sense Disambiguation(WSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46077eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for ss in wordnet.synsets('mouse'):\n",
    "    print(ss, ss.definition())\n",
    "    \n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"------------WSD---------------\")\n",
    "print(\"For bass: \")\n",
    "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), \"bass\")\n",
    "print(context_1, context_1.definition())\n",
    "\n",
    "context_2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), \"bass\")\n",
    "print(context_2, context_2.definition())\n",
    "\n",
    "print(\"For mouse: \")\n",
    "context_3 = lesk(word_tokenize(\"My mouse is not working, need to change it\"), \"mouse\")\n",
    "print(context_3, context_3.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240de0f",
   "metadata": {},
   "source": [
    "# 5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26222009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning\\\n",
    "             with python. All pythoners have pythoned poorly at least once.\"\n",
    "             \n",
    "l_s =  LancasterStemmer()\n",
    "stem_lan =  [l_s.stem(word) for word in word_tokenize(new_text)]\n",
    "print(stem_lan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5595b",
   "metadata": {},
   "source": [
    "# 6. Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'Text':corpus})\n",
    "print(df)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v = CountVectorizer()\n",
    "X = count_v.fit_transform(df.Text).toarray()\n",
    "print(count_v.get_feature_names())\n",
    "\n",
    "print(X)\n",
    "print(count_v.vocabulary_)\n",
    "\n",
    "count_v = CountVectorizer(stop_words=['this','is'])\n",
    "X = count_v.fit_transform(df.Text).toarray()\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a11b5e",
   "metadata": {},
   "source": [
    "# 7. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c450dd7",
   "metadata": {},
   "source": [
    "# 8. Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098dc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'Text':corpus})\n",
    "\n",
    "hash_v = HashingVectorizer(n_features=15, norm=None, alternate_sign=True)\n",
    "print(hash_v.fit_transform(df.Text).toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
